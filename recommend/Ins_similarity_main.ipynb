{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import statistics\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#from dataprep.eda import plot\n",
    "#import cx_Oracle\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "from matplotlib import font_manager, rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances  #cosine 유사도 구할때 사용\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score,classification_report,f1_score,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import pairwise_distances  #cosine 유사도 구할때 사용\n",
    "\n",
    "import surprise\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from surprise import CoClustering\n",
    "from surprise import SlopeOne\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import accuracy\n",
    "from surprise import KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split as train_test_split_su\n",
    "from surprise import BaselineOnly\n",
    "from surprise import NMF\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set option\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "plt.rc(\"font\", family=\"Malgun Gothic\") # 한글깨짐현상 방지 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 cosine 유사도 - 사용자간 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.4.1 사용자 유사도를 위해 데이터 분포 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_20220422 df_add.to_csv('data_20220422/data.csv', index=False, encoding = 'utf8') \n",
    "df_add = pd.read_csv('data.csv',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add['최종학력코드'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.loc[(df_add['나이'] >= 20) & (df_add['나이'] < 30) , '연령대'] = 20\n",
    "df_add.loc[(df_add['나이'] >= 30) & (df_add['나이'] < 40) , '연령대'] = 30\n",
    "df_add.loc[(df_add['나이'] >= 40) & (df_add['나이'] < 50) , '연령대'] = 40\n",
    "df_add.loc[(df_add['나이'] >= 50) & (df_add['나이'] < 60) , '연령대'] = 50\n",
    "df_add.loc[(df_add['나이'] >= 60) & (df_add['나이'] < 70) , '연령대'] = 60\n",
    "df_add.loc[(df_add['나이'] >= 10) & (df_add['나이'] < 20) , '연령대'] = 10\n",
    "df_add.loc[(df_add['나이'] < 10) , '연령대'] = 0\n",
    "df_add.loc[(df_add['나이'] >= 70) ,'연령대'] = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_add.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 FEATURE 추출\n",
    "df_cos = df_add[['고객번호','성별구분','연령대','나이','직업위험등급','직업대분류코드'\n",
    "                 ,'배우자여부','자녀여부','설계사수' ,'경제활동참가율'#'지점수','설계사수'\n",
    "                 ,'스타벅스매장수','2020_건강생활실천율','시도명','상품분류코드_1'#,'유지계약건수','성립계약건수'\n",
    "                 #'상급종합병원','2020_건강생활실천율','2020_출생률' ,'평균연령'\n",
    "                 # 'FP몰선호상품코드','모집대면비대면여부',\n",
    "                 #,'2020_노인여가복지시설수' ,'학문/교육' \n",
    "                ]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos_label = df_cos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직업대분류 코드 LABEL 화 \n",
    "df_cos_label = pd.get_dummies(df_cos_label, columns = ['직업대분류코드'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cos = df_cos_label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp DF생성 \n",
    "df_add_temp = df_add[['고객번호','상품분류코드_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_cus_index = df_add_temp.set_index('고객번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_add_temp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_cus_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최근 많이 판매된 상품 top3 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil_sal = (df_add.계약월_1 >= 202106)   #data_final\n",
    "df_add_top_sal = df_add.loc[fil_sal,:]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_top_sal.groupby('계약월_1').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_sal = df_add_top_sal['상품분류코드_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sal_list  = []\n",
    "top_sal_list = df_top_sal.value_counts().index[:3]\n",
    "top_sal_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top_sal.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### knn data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 FEATURE 추출\n",
    "df_knn = df_add[['고객번호','성별구분','나이','직업위험등급','직업대분류코드'\n",
    "                 ,'배우자여부','자녀여부','설계사수','보건진료소' #'지점수','설계사수'\n",
    "                 ,'스타벅스매장수','2020_건강생활실천율','시도명', '경제활동참가율'\n",
    "                 #'상급종합병원','2020_건강생활실천율','2020_출생률' # 경제활동참가율\n",
    "                 # 'FP몰선호상품코드','모집대면비대면여부',,'평균연령'\n",
    "                 #,'2020_노인여가복지시설수' ,'학문/교육' \n",
    "                ]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직업대분류 코드 LABEL 화 \n",
    "df_knn = pd.get_dummies(df_knn, columns = ['직업대분류코드'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시도명 label화\n",
    "df_knn = pd.get_dummies(df_knn, columns = ['시도명'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_copy = df_knn[['고객번호']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_copy.to_csv('df_knn_copy_random.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn = df_knn.drop(['고객번호'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 진행\n",
    "scaler = MinMaxScaler()\n",
    "data_scale = scaler.fit_transform(df_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그룹수를 50개 정도로 조정 \n",
    "k = 50\n",
    "\n",
    "# 그룹 수, random_state 설정\n",
    "model = KMeans(init = 'random' , n_clusters = k ) #init = 'k-means++'random random_state = 10\n",
    "\n",
    "# 정규화된 데이터에 학습\n",
    "model.fit(data_scale)\n",
    "\n",
    "# 클러스터링 결과 각 데이터가 몇 번째 그룹에 속하는지 저장\n",
    "df_knn['cluster'] = model.fit_predict(data_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data (df_cos, option ,sex,age,addr,k,size):\n",
    "    if option == 'human':\n",
    "        # 들어온 값을 시도 한글로 변경 \n",
    "        if addr == 0 :\n",
    "            addr = '서울특별시' \n",
    "        elif addr == 1 :\n",
    "            addr = '부산광역시'         \n",
    "        elif addr == 2 :\n",
    "            addr = '대구광역시'\n",
    "        elif addr == 3 :\n",
    "            addr = '인천광역시'\n",
    "        elif addr == 4 :\n",
    "            addr = '광주광역시'\n",
    "        elif addr == 5 :\n",
    "            addr = '대전광역시'\n",
    "        elif addr == 6 :\n",
    "            addr = '울산광역시'\n",
    "        elif addr == 7 :\n",
    "            addr = '세종특별자치시'\n",
    "        elif addr == 8 :\n",
    "            addr = '경기도'\n",
    "        elif addr == 9 :\n",
    "            addr = '강원도'\n",
    "        elif addr == 10 :\n",
    "            addr = '충청북도'            \n",
    "        elif addr == 11 :\n",
    "            addr = '충청남도'  \n",
    "        elif addr == 12 :\n",
    "            addr = '전라북도'              \n",
    "        elif addr == 13 :\n",
    "            addr = '전라남도'             \n",
    "        elif addr == 14 :\n",
    "            addr = '경상북도'              \n",
    "        elif addr == 15 :\n",
    "            addr = '경상남도'             \n",
    "        elif addr == 16 :\n",
    "            addr = '제주특별자치도'\n",
    "        else:\n",
    "            print('시도 error : ', addr)\n",
    "          \n",
    "        print('성별: {0}, 연령대 : {1}, 시도명 : {2}'.format(sex,age,addr))\n",
    "        #fil_yn = '##'\n",
    "        #if age >= 60:\n",
    "        #    fil_yn = (df_cos.성별구분 == sex) & (df_cos.연령대 >= age)  & (df_cos.시도명 == addr)  \n",
    "        #else: \n",
    "        fil_yn = (df_cos.성별구분 == sex) & (df_cos.연령대 == age)  & (df_cos.시도명 == addr)  #data_final\n",
    "        filename = 'df_cos_{0}_{1}_{2}'.format(sex,age,addr)\n",
    "       # print(filename)\n",
    "        data = df_cos.loc[fil_yn,:]\n",
    "        data = data.drop(['성별구분','연령대','시도명'],axis = 1 )\n",
    "    elif option == 'knn':\n",
    "        fil_yn = (df_knn.cluster == k)   #data_final\n",
    "        filename = df_knn.loc[fil_yn,:]   \n",
    "        data = pd.merge(left = filename , right = df_knn_copy, left_index=True, right_index=True, how='left')\n",
    "        data = data.drop(['cluster'],axis=1)\n",
    "    else :\n",
    "        data = pd.get_dummies(df_cos, columns = ['시도명'])\n",
    "        data = data.sample(n=size,replace=False) # replace=True 비복원추출 \n",
    "    #print(\"data.shape\",data.shape)    \n",
    "    return data    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_reco(data):\n",
    "    #print(\"data_shape\",data.shape)\n",
    "    #----------cosine 유사도를 구하기 위해 StandardScaler start -----------------------\n",
    "    data = data.set_index('고객번호',drop=True)\n",
    "    print(\"StandardScaler start! \")\n",
    "    # StandardScaler 사용 \n",
    "    # 객체생성\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # fit -> transform\n",
    "    scaler.fit(data) # df는 2차원 이상의 값이어야 함\n",
    "    df_scaled = scaler.transform(data)\n",
    "\n",
    "    # 배열형태로 반환되기 때문에 데이터 프레임으로 변환해주는 작업\n",
    "    df_scaled = pd.DataFrame(data = df_scaled, index=data.index, columns=data.columns)\n",
    "\n",
    "    # *** 만약 특정 열의 스케일링을 하고 싶은 경우 ***\n",
    "    #A_n = scaler.fit_transform(df['A'].values.reshape(-1,1))\n",
    "    #df.insert(0, 'A_scaled', A_n)\n",
    "    #df.drop(['A'], axis=1, inplace=True)\n",
    "    \n",
    "    # 파일 copy \n",
    "    df = df_scaled.copy()\n",
    "    print(\"StandardScaler End! \")\n",
    "    #----------cosine 유사도를 구하기 위해 StandardScaler end -----------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------cosine 유사도 start --------------------------------------------------\n",
    "    print(\"cosine_similarity start! \")\n",
    "    item_sim = cosine_similarity(df, df)\n",
    "\n",
    "    # cosine_similarity() 로 반환된 넘파이 행렬을 영화명을 매핑하여 DataFrame으로 변환\n",
    "    item_sim_df = pd.DataFrame(data=item_sim, index=df.index,\n",
    "                           columns=df.index)\n",
    "    #print(item_sim_df.shape)\n",
    "    item_sim_df.head(10)\n",
    "    \n",
    "    df = item_sim_df.copy()\n",
    "    print(\"cosine_similarity End! \")\n",
    "    #----------cosine 유사도 end----0-------------------------------------------------\n",
    "    \n",
    "    #----------유사한 고객 산출 start--------------------------------------------------\n",
    "    print(\"cosine_customer start! \")\n",
    "    # 30개 추출 \n",
    "    dic = {}\n",
    "    for 고객번호 in df.columns:\n",
    "        유사집단 = df.loc[int(고객번호), df.columns != 고객번호].sort_values(\n",
    "            ascending=False).head(30).index.tolist()\n",
    "        dic[고객번호]=유사집단\n",
    "        \n",
    "    df_tec = pd.DataFrame(dic).transpose()    \n",
    "    #print('df_tec.shape',df_tec.shape)\n",
    "    df = df_tec.copy()\n",
    "    print(\"cosine_customer end! \")\n",
    "    #----------유사한 고객 산출 end---------------------------------------------------\n",
    "\n",
    "    #----------유사한 고객의 상품코드 추출 start -------------------------------------\n",
    "    print(\"cosine_prod start! \")\n",
    "    df = df.iloc[:,:]\n",
    "    dic = {}\n",
    "    keys = df.columns.tolist()\n",
    "    for key in keys:\n",
    "        dic[key] = df[key].tolist()\n",
    "    \n",
    "    고객번호_dic = {}\n",
    "    고객번호_list = []\n",
    "    keys = df.columns.tolist()\n",
    "    #for index, 고객번호 in enumerate(df['고객번호'].tolist()):\n",
    "    for index, 고객번호 in enumerate(df.index.tolist()):\n",
    "        고객번호_dic[고객번호] = df.iloc[index].tolist()\n",
    "    \n",
    "    recommendation_dic = {}\n",
    "    for key in 고객번호_dic.keys():\n",
    "        #print(f'고객번호 : {key}')\n",
    "        recommendation_list = []\n",
    "        for index in range(len(고객번호_dic[key])):\n",
    "            recommendation = df2.loc[df2['고객번호']==고객번호_dic[key][index], '상품분류코드_1'].values[0]\n",
    "            recommendation_list.append(recommendation)\n",
    "        recommendation_dic[key] = recommendation_list\n",
    "   # print('df_rec.shape',df_rec.shape)\n",
    "    pd.DataFrame(recommendation_dic).transpose()\n",
    "    df_rec = pd.DataFrame(recommendation_dic).transpose()\n",
    "    print(\"cosine_prod end! \")\n",
    "    #----------유사한 고객의 상품코드 추출 end ----------------------------------------\n",
    "    \n",
    "    \n",
    "    #----------유사한 고객의 상품코드 최빈값 start -------------------------------------\n",
    "    print(\"customer mode start! \")\n",
    "    # mode 최빈값 여러건인지 확인하기 \n",
    "    df_rec['추천코드_1'] = '0'\n",
    "    df_rec['추천코드_2'] = '0'\n",
    "    df_rec['추천코드_3'] = '0'\n",
    "    df_rec['추천코드_top'] = '0'\n",
    "    # top_sal_list\n",
    "    k = 1 \n",
    "    for i in range (0,len(df_rec)):\n",
    "        list = []\n",
    "        list = str(statistics.mode(df_rec.iloc[i]))\n",
    "        #print(list)\n",
    "        \n",
    "        df_rec[\"추천코드_1\"].iloc[i] = str(statistics.mode(df_rec.iloc[i]))\n",
    "        #print(str(statistics.mode(df_rec.iloc[i])))\n",
    "        # 두번째 추천코드 \n",
    "        # 두번째 추천코드\n",
    "        if str(statistics.mode(df_rec.iloc[i])) != '0' :\n",
    "            fil = (statistics.mode(df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]) \n",
    "                   != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i])\n",
    "            prd_2 = df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i][fil]\n",
    "            if len(prd_2) != 0:\n",
    "                df_rec[\"추천코드_2\"].iloc[i]= str(statistics.mode(prd_2))\n",
    "                # -----3번쨰 상품 추천을 위해 \n",
    "                fil3 = ((statistics.mode(df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]) \n",
    "                        != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]))\n",
    "                fil4 = statistics.mode(prd_2) != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i] \n",
    "            \n",
    "                fil_3 = ( fil3 == fil4)  \n",
    "                prd_3 =  df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i][fil_3]\n",
    "                \n",
    "                if len(prd_3)!= 0:\n",
    "                    df_rec[\"추천코드_3\"].iloc[i]= str(statistics.mode(prd_3))\n",
    "                    #print(str(statistics.mode(prd_3)))\n",
    "                else:\n",
    "                    df_rec[\"추천코드_3\"].iloc[i] = 0\n",
    "                \n",
    "            else :\n",
    "                df_rec[\"추천코드_2\"].iloc[i] = 0\n",
    "        else :\n",
    "            df_rec[\"추천코드_2\"].iloc[i] = 0\n",
    "        \n",
    "\n",
    "        for u in range (0,3):\n",
    "                     \n",
    "            if (df_rec[\"추천코드_1\"].iloc[i] != str(top_sal_list[u])) and (df_rec[\"추천코드_2\"].iloc[i] != str(top_sal_list[u])):\n",
    "                df_rec['추천코드_top'].iloc[i] =  str(top_sal_list[u]) \n",
    "                break\n",
    "                \n",
    "        \n",
    "        #if str(statistics.mode(df_rec.iloc[i])) != '0' :\n",
    "        #    #print(\"aaa\")\n",
    "        #    fil = (statistics.mode(df_rec.iloc[i]) != df_rec.iloc[i])\n",
    "        #    prd_2 = df_rec.iloc[i][fil]\n",
    "        #    df_rec[\"추천코드_2\"].iloc[i]= str(statistics.mode(prd_2))\n",
    "        #else :\n",
    "        #    df_rec[\"추천코드_2\"].iloc[i] = 0\n",
    "        #    \n",
    "        if len(list) > 3 :\n",
    "            print(\"2개이상\" , i)\n",
    "            k = k + 1\n",
    "        i = i+1\n",
    "    #print(k)\n",
    "    print(\"customer mode end! \")\n",
    "    #----------유사한 고객의 상품코드 최빈값 end -------------------------------------\n",
    "    \n",
    "    #-----------추천 상품에 대한 정확도 측정 start ------------------------------------\n",
    "   # print(\"correct start! \")\n",
    "    \n",
    "    df_t = pd.merge(left = df_rec , right = df_add_cus_index, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    df_t['일치여부_1'] = 0\n",
    "    df_t['일치여부_2'] = 0\n",
    "    df_t['일치여부_3'] = 0   \n",
    "    df_t['일치여부_top'] = 0\n",
    "    match1_y = 0\n",
    "    match2_y = 0\n",
    "    match3_y = 0\n",
    "    match_top_y = 0 \n",
    "    for i in range (0,len(df_t)): #len(df_t)\n",
    "\n",
    "        if str(df_t['추천코드_1'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "            df_t['일치여부_1'].iloc[i] = 1\n",
    "            match1_y = match1_y+1\n",
    "        else:\n",
    "            df_t['일치여부_1'].iloc[i] = 0        \n",
    "        \n",
    "        if str(df_t['추천코드_2'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "            df_t['일치여부_2'].iloc[i] = 1\n",
    "            match2_y = match2_y + 1\n",
    "        else:\n",
    "            df_t['일치여부_2'].iloc[i] = 0      \n",
    "        \n",
    "        if str(df_t['추천코드_3'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "            df_t['일치여부_3'].iloc[i] = 1\n",
    "            match3_y = match3_y + 1\n",
    "        else:\n",
    "            df_t['일치여부_3'].iloc[i] = 0   \n",
    "            \n",
    "        if str(df_t['추천코드_top'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "            df_t['일치여부_top'].iloc[i] = 1\n",
    "            match_top_y = match_top_y + 1\n",
    "        else:\n",
    "            df_t['일치여부_top'].iloc[i] = 0              \n",
    "    #print(match1_y/len(df_t), '/',match1_y, len(df_t))\n",
    "    #print(match2_y/len(df_t), '/',match2_y, len(df_t))\n",
    "    #print(\"correct end! \")\n",
    "    return match1_y, match2_y,match3_y,match_top_y, len(df_t)\n",
    "    #return df_t \n",
    "    \n",
    "    #-----------추천 상품에 대한 정확도 측정 end ------------------------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델별 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_result = pd.DataFrame(columns=['idx', 'knn_cluster','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### k-nn 클러스터별 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,50):\n",
    "    corr_rate = 0 \n",
    "    data = make_data(df_knn,'knn',0 ,0 ,0,i,0) ## 앞에 어느 값을 줄지 선택 \n",
    "    match1_y, match2_y, match3_y,match_top_y, df_len = cos_reco(data) \n",
    "    knn_cluster = i \n",
    "    idx = i \n",
    "    corr_rate  = round((int(match1_y)+int(match2_y)+int(match3_y)) / int(df_len), 6) \n",
    "    df_knn_result = df_knn_result.append(pd.DataFrame([[idx, knn_cluster,match1_y,match2_y,match3_y,match_top_y,df_len,corr_rate]],\n",
    "                                   columns=['idx', 'knn_cluster','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate']), \n",
    "                                   ignore_index=True)\n",
    "    #print('i',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_knn_result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn_result.to_csv('df_random_20220520_v3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사람이 정한 클러스터로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = pd.DataFrame(columns=['idx', 'sex','years','region','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range (1,3): # 성별 (1,2) # 3\n",
    "    corr_rate = 0 \n",
    "    for k in range (1,7): # 8 --> 연령대 \n",
    "        for u in range (0,17): # 18  --> 시도 \n",
    "            data = make_data(df_cos,'human',j ,k*10 ,u,0,0) ## 앞에 어느 값을 줄지 선택  data = make_data(df_cos,'human',1,30,'서울특별시',1)\n",
    "            if len(data) != 0 :\n",
    "                # 유사도 측정 \n",
    "                match1_y, match2_y, match3_y,match_top_y, df_len = cos_reco(data)  \n",
    "                # 시도 setting \n",
    "                idx = (j + k + u )\n",
    "                sex = j\n",
    "                years = (k*10)\n",
    "                region = u\n",
    "                # 정확도 측정 \n",
    "                corr_rate  = round((int(match1_y)+int(match2_y)+int(match3_y)) / int(df_len), 6) \n",
    "                df_h = df_h.append(pd.DataFrame([[idx, sex,years,region,match1_y,match2_y,match3_y,match_top_y,df_len,corr_rate]],\n",
    "                                            columns=['idx', 'sex','years','region','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate']), \n",
    "                                            ignore_index=True)\n",
    "            else:\n",
    "                print(\"pass\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h.to_csv('df_h_20220521.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model기반 추천 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요 FEATURE 추출\n",
    "df_DT  = df_add[['고객번호','성별구분','나이','직업위험등급','직업대분류코드'\n",
    "                 ,'배우자여부','자녀여부','설계사수','보건진료소' #'지점수','설계사수'\n",
    "                 ,'스타벅스매장수','2020_건강생활실천율','시도명', '경제활동참가율'\n",
    "                 ,'상품분류코드_1'\n",
    "                 #'상급종합병원','2020_건강생활실천율','2020_출생률' ,,'평균연령'\n",
    "                 # 'FP몰선호상품코드','모집대면비대면여부',\n",
    "                 #,'2020_노인여가복지시설수' ,'학문/교육' \n",
    "                ]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직업대분류 코드 LABEL 화 \n",
    "df_DT = pd.get_dummies(df_DT, columns = ['직업대분류코드'])\n",
    "# 시도명 label화\n",
    "df_DT = pd.get_dummies(df_DT, columns = ['시도명'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_DT.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(['상품분류코드_1'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = df['상품분류코드_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 고객번호로\n",
    "df = df.set_index('고객번호')\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df_x, df_y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    max_depths = trial.suggest_int('max_depth', 1, 9, 1)\n",
    "   # n_estimators = trial.suggest_int('n_estimators', 1, 20, 1)\n",
    "    max_leaf_nodes = int(trial.suggest_int(\"max_leaf_nodes\", 2, 50))\n",
    "\n",
    "    clf = DecisionTreeClassifier(#n_estimators=n_estimators,\n",
    "                                 max_leaf_nodes = max_leaf_nodes,                 \n",
    "                                 criterion=criterions,\n",
    "                                 max_depth=max_depths)\n",
    "                               #  n_jobs=-1)\n",
    "                                 \n",
    "    score = cross_val_score(clf, x_train, y_train, scoring=\"accuracy\").mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
    "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# 시행된 trial 중 가장 높은 값 반환하는 메소드\n",
    "optuna_acc = study.best_trial.value\n",
    "print(optuna_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(max_depth=6, max_leaf_nodes = 36, splitter='best', criterion='entropy')\n",
    "dtc.fit(x_train, y_train)\n",
    "dtc_pred = dtc.predict(x_valid)\n",
    "(dtc_pred == y_valid).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df for valid(test)\n",
    "df_valid = x_valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상품분류코드 idx\n",
    "dtc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_code = {0: 111,\n",
    "             1: 120,\n",
    "             2: 130,\n",
    "             3: 141,\n",
    "             4: 142,\n",
    "             5: 150,\n",
    "             6: 162,\n",
    "             7: 163,\n",
    "             8: 164,\n",
    "             9: 181,\n",
    "             10: 182,\n",
    "             11: 183,\n",
    "             12: 192}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba 분류기 확률값 치환\n",
    "dtc_pred_proba = dtc.predict_proba(x_valid)\n",
    "dtc_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증, 첫번째 분류기 output값이 150이 맞는지 확인해보자\n",
    "a = dtc_pred_proba[0]\n",
    "\n",
    "# np.argpartition(sth, 2) -> 순서 상관 없이 2번째까지 작은 숫자 idx 뽑아서 왼쪽으로 놓는다\n",
    "# np.argpartition(sth, -2) -> 순서 상관 없이 2번째까지 큰 숫자 idx 뽑아서 오른쪽으로 놓는다\n",
    "\n",
    "# 2번째까지 큰 숫자 idx 뽑아서 오른쪽으로 놓고\n",
    "# 오른쪽 2개를 idx로 가져온다\n",
    "idx = np.argpartition(a, -3)[-3:] # idx = [rank2, rank1]\n",
    "\n",
    "# idx는 상품분류코드 번호를 뜻하게 된다\n",
    "# reverse -> [rank1, rank2]\n",
    "idx = np.sort(idx)[::-1]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_valid(test) 생성\n",
    "df_valid = x_valid.copy()\n",
    "\n",
    "# rank 컬럼 생성\n",
    "df_valid['rank1']=''\n",
    "df_valid['rank2']=''\n",
    "df_valid['rank3']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, 고객번호 in enumerate(df_valid.index):\n",
    "    추천상품 = np.argpartition(dtc_pred_proba[index], -3)[-3:]\n",
    "    df_valid.loc[고객번호, 'rank1'] = 추천상품[0]\n",
    "    df_valid.loc[고객번호, 'rank2'] = 추천상품[1]\n",
    "    df_valid.loc[고객번호, 'rank3'] = 추천상품[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상품분류코드로 변환하기\n",
    "df_valid['rank1'] = df_valid['rank1'].replace(item_code)\n",
    "df_valid['rank2'] = df_valid['rank2'].replace(item_code)\n",
    "df_valid['rank3'] = df_valid['rank3'].replace(item_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% 테스트 \n",
    "df_valid_DT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT_cus = df_add[['고객번호','상품분류코드_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT_cus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT_cus = df_DT_cus.set_index('고객번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_DT = df_valid.set_index('고객번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DT_cus.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_DT.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.merge(left = df_valid_DT , right = df_DT_cus, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['일치여부_1'] = 0\n",
    "df_t['일치여부_2'] = 0\n",
    "df_t['일치여부_3'] = 0\n",
    "match1_y = 0\n",
    "match2_y = 0\n",
    "match3_y = 0\n",
    "for i in range (0,len(df_t)): #len(df_t)\n",
    "\n",
    "    if str(df_t['rank1'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "        df_t['일치여부_1'].iloc[i] = 1\n",
    "        match1_y = match1_y+1\n",
    "    else:\n",
    "        df_t['일치여부_1'].iloc[i] = 0        \n",
    "    \n",
    "    if str(df_t['rank2'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "        df_t['일치여부_2'].iloc[i] = 1\n",
    "        match2_y = match2_y + 1\n",
    "    else:\n",
    "        df_t['일치여부_2'].iloc[i] = 0 \n",
    "\n",
    "    if str(df_t['rank3'].values[i]) == str(df_t['상품분류코드_1'].values[i]):\n",
    "        df_t['일치여부_3'].iloc[i] = 1\n",
    "        match3_y = match3_y + 1\n",
    "    else:\n",
    "        df_t['일치여부_3'].iloc[i] = 0         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match1_y, match2_y, len(df_t)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(match1_y + match2_y) / len(df_t)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparamete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import sklearn\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 10)\n",
    "    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)\n",
    "    n_estimators =  trial.suggest_int('n_estimators', 100, 500)\n",
    "\n",
    "    clf = sklearn.ensemble.RandomForestClassifier( criterion=criterions,\n",
    "                                 n_estimators=n_estimators,\n",
    "                                 max_leaf_nodes = max_leaf_nodes,                 \n",
    "                                 max_depth=max_depth,\n",
    "                                 n_jobs=-1,\n",
    "                                 random_state=0                 \n",
    "                                                 )\n",
    "                                 \n",
    "    score = cross_val_score(clf, x_train, y_train, scoring=\"accuracy\").mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계\n",
    "# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "trial = study.best_trial\n",
    "\n",
    "print('Accuracy: {}'.format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드\n",
    "print(study.best_trial.params)\n",
    "\n",
    "# 시행된 trial 중 가장 높은 값 반환하는 메소드\n",
    "optuna_acc = study.best_trial.value\n",
    "print(optuna_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=8,max_leaf_nodes = 733, n_estimators= 500, criterion='entropy', random_state=0) \n",
    "# (n_estimators=474, max_depth=1, max_leaf_nodes=552, random_state=25)\n",
    "rfc.fit(x_train, y_train)\n",
    "rfc_pred = rfc.predict(x_valid)\n",
    "(rfc_pred == y_valid).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_proba 분류기 확률값 치환\n",
    "rfc_pred_proba = rfc.predict_proba(x_valid)\n",
    "rfc_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, 고객번호 in enumerate(df_valid.index):\n",
    "    추천상품 = np.argpartition(rfc_pred_proba[index], -3)[-3:]\n",
    "    df_valid.loc[고객번호, 'rank1'] = 추천상품[0]\n",
    "    df_valid.loc[고객번호, 'rank2'] = 추천상품[1]\n",
    "    df_valid.loc[고객번호, 'rank3'] = 추천상품[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상품분류코드로 변환하기\n",
    "df_valid['rank1'] = df_valid['rank1'].replace(item_code)\n",
    "df_valid['rank2'] = df_valid['rank2'].replace(item_code)\n",
    "df_valid['rank3'] = df_valid['rank3'].replace(item_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.to_csv('df_valid_RF.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_RF = pd.read_csv('df_valid_RF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_RF = df_valid.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_RF = df_valid_RF.set_index('고객번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = pd.merge(left = df_valid_RF , right = df_DT_cus, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1['일치여부_1'] = 0\n",
    "df_t1['일치여부_2'] = 0\n",
    "df_t1['일치여부_3'] = 0\n",
    "\n",
    "match1_y = 0\n",
    "match2_y = 0\n",
    "match3_y = 0\n",
    "\n",
    "for i in range (0,len(df_t1)): #len(df_t)\n",
    "\n",
    "    if str(df_t1['rank1'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):\n",
    "        df_t1['일치여부_1'].iloc[i] = 1\n",
    "        match1_y = match1_y+1\n",
    "    else:\n",
    "        df_t1['일치여부_1'].iloc[i] = 0        \n",
    "    \n",
    "    if str(df_t1['rank2'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):\n",
    "        df_t1['일치여부_2'].iloc[i] = 1\n",
    "        match2_y = match2_y + 1\n",
    "    else:\n",
    "        df_t1['일치여부_2'].iloc[i] = 0 \n",
    "        \n",
    "    if str(df_t1['rank3'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):\n",
    "        df_t1['일치여부_3'].iloc[i] = 1\n",
    "        match3_y = match3_y + 1\n",
    "    else:\n",
    "        df_t1['일치여부_3'].iloc[i] = 0         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match1_y, match2_y, len(df_t)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(match1_y+ match2_y) / len(df_t)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 협업필터링(사용자간유사도_사용자기반_상품가입건_유)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_add[['고객번호','상품분류코드_1','상품분류코드_2','상품분류코드_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자간 선호도를 상품 종류 코드에 따라 가입건수 count로 설정 \n",
    "df_sim = pd.get_dummies(df_sim, columns = ['상품분류코드_1'])\n",
    "df_sim = pd.get_dummies(df_sim, columns = ['상품분류코드_2'])\n",
    "df_sim = pd.get_dummies(df_sim, columns = ['상품분류코드_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim['상품분류코드_111'] = df_sim['상품분류코드_1_111'] + df_sim['상품분류코드_2_111.0'] + df_sim['상품분류코드_3_111.0']\n",
    "df_sim['상품분류코드_120'] = df_sim['상품분류코드_1_120'] + df_sim['상품분류코드_2_120.0'] + df_sim['상품분류코드_3_120.0']\n",
    "df_sim['상품분류코드_130'] = df_sim['상품분류코드_1_130'] + df_sim['상품분류코드_2_130.0'] + df_sim['상품분류코드_3_130.0']\n",
    "df_sim['상품분류코드_141'] = df_sim['상품분류코드_1_141'] + df_sim['상품분류코드_2_141.0'] + df_sim['상품분류코드_3_141.0']\n",
    "df_sim['상품분류코드_142'] = df_sim['상품분류코드_1_142'] + df_sim['상품분류코드_2_142.0'] + df_sim['상품분류코드_3_142.0']\n",
    "df_sim['상품분류코드_150'] = df_sim['상품분류코드_1_150'] + df_sim['상품분류코드_2_150.0'] + df_sim['상품분류코드_3_150.0']\n",
    "df_sim['상품분류코드_162'] = df_sim['상품분류코드_1_162'] + df_sim['상품분류코드_2_162.0'] + df_sim['상품분류코드_3_162.0']\n",
    "df_sim['상품분류코드_163'] = df_sim['상품분류코드_1_163'] + df_sim['상품분류코드_2_163.0'] + df_sim['상품분류코드_3_163.0']\n",
    "df_sim['상품분류코드_164'] = df_sim['상품분류코드_1_164'] + df_sim['상품분류코드_2_164.0'] + df_sim['상품분류코드_3_164.0']\n",
    "df_sim['상품분류코드_181'] = df_sim['상품분류코드_1_181'] + df_sim['상품분류코드_2_181.0'] + df_sim['상품분류코드_3_181.0']\n",
    "df_sim['상품분류코드_182'] = df_sim['상품분류코드_1_182'] + df_sim['상품분류코드_2_182.0'] \n",
    "df_sim['상품분류코드_183'] = df_sim['상품분류코드_1_183'] + df_sim['상품분류코드_2_183.0'] + df_sim['상품분류코드_3_183.0']\n",
    "df_sim['상품분류코드_192'] = df_sim['상품분류코드_1_192'] + df_sim['상품분류코드_2_192.0'] + df_sim['상품분류코드_3_192.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_sim[['고객번호',\n",
    "                 '상품분류코드_111',\n",
    "                 '상품분류코드_120',\n",
    "                 '상품분류코드_130',\n",
    "                 '상품분류코드_141',\n",
    "                 '상품분류코드_142',\n",
    "                 '상품분류코드_150',\n",
    "                 '상품분류코드_162',\n",
    "                 '상품분류코드_163',\n",
    "                 '상품분류코드_164',\n",
    "                 '상품분류코드_181',\n",
    "                 '상품분류코드_182',\n",
    "                 '상품분류코드_183',\n",
    "                 '상품분류코드_192' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim = df_sim.rename(columns = {'상품분류코드_111':'111'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_120':'120'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_130':'130'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_141':'141'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_142':'142'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_150':'150'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_162':'162'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_163':'163'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_164':'164'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_181':'181'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_182':'182'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_183':'183'})\n",
    "df_sim = df_sim.rename(columns = {'상품분류코드_192':'192'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_sim.melt(id_vars = \"고객번호\",var_name = \"prodId\",value_name = \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns = {'고객번호':'userId'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.groupby('userId').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df1, df2, how='inner', on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'prodId_x': 'prodId', 'rating_x': 'rating_1', 'rating_y':'rating_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no of ratings :\",df.shape[0])\n",
    "print(\"No. of unique users:\", df[\"userId\"].nunique())\n",
    "print(\"No. of unique movies:\", df[\"prodId\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import KFold\n",
    "\n",
    "\n",
    "def precision_recall_at_k(predictions, k=3, threshold=1.0):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "    #print(\"predictions : \",predictions)\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, prod_cd, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "        \n",
    "     #   print(\"input: \", uid,prod_cd,true_r,est,prod_cd)\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        #print(\"aa:\",user_ratings.items)\n",
    "        #print(\"user_ratings:\",user_ratings)\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "        #print(\"user_ratings[:k]\",user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "reader = Reader()\n",
    "ratings = Dataset.load_from_df(df[['userId', 'prodId', 'rating_1']], reader)\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(ratings):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=3, threshold=1.0)\n",
    "\n",
    "    # Precision and recall can then be averaged over all users\n",
    "    print(sum(prec for prec in precisions.values()) / len(precisions))\n",
    "    print(sum(rec for rec in recalls.values()) / len(recalls))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
