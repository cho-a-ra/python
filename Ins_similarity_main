import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os 
import math
import pickle
import random
import statistics
#pd.set_option('display.max_columns', None)
#pd.set_option('display.max_rows', None)
#from dataprep.eda import plot
#import cx_Oracle


import warnings


import optuna
from optuna import Trial, visualization
from optuna.samplers import TPESampler


import lightgbm as lgb
from matplotlib import font_manager, rc
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn import tree
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
from sklearn.tree import export_graphviz
from sklearn.model_selection import GridSearchCV

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import cross_val_score
from sklearn.metrics.pairwise import pairwise_distances  #cosine 유사도 구할때 사용

from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split 
from sklearn.metrics import confusion_matrix, precision_score, recall_score,classification_report,f1_score,roc_auc_score
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import pairwise_distances  #cosine 유사도 구할때 사용

import surprise
from surprise import SVD
from surprise.model_selection import GridSearchCV,RandomizedSearchCV
from surprise import CoClustering
from surprise import SlopeOne
from surprise import Dataset
from surprise import Reader
from surprise import accuracy
from surprise import KNNBasic
from surprise.model_selection import cross_validate
from surprise.model_selection import train_test_split as train_test_split_su
from surprise import BaselineOnly
from surprise import NMF
from surprise.model_selection import KFold

from collections import defaultdict


import warnings
warnings.filterwarnings('ignore')

# set option
pd.set_option('display.max_rows', 50)
pd.set_option('display.max_columns', 200)
pd.set_option('display.precision', 2)
pd.options.display.float_format = '{:.4f}'.format

plt.rc("font", family="Malgun Gothic") # 한글깨짐현상 방지 

#3.2.4 cosine 유사도 - 사용자간 유사도
#3.2.4.1 사용자 유사도를 위해 데이터 분포 파악
#data_20220422 df_add.to_csv('data_20220422/data.csv', index=False, encoding = 'utf8') 
df_add = pd.read_csv('data.csv',encoding = 'utf-8')

df_add.loc[(df_add['나이'] >= 20) & (df_add['나이'] < 30) , '연령대'] = 20
df_add.loc[(df_add['나이'] >= 30) & (df_add['나이'] < 40) , '연령대'] = 30
df_add.loc[(df_add['나이'] >= 40) & (df_add['나이'] < 50) , '연령대'] = 40
df_add.loc[(df_add['나이'] >= 50) & (df_add['나이'] < 60) , '연령대'] = 50
df_add.loc[(df_add['나이'] >= 60) & (df_add['나이'] < 70) , '연령대'] = 60
df_add.loc[(df_add['나이'] >= 10) & (df_add['나이'] < 20) , '연령대'] = 10
df_add.loc[(df_add['나이'] < 10) , '연령대'] = 0
df_add.loc[(df_add['나이'] >= 70) ,'연령대'] = 70

# 중요 FEATURE 추출
df_cos = df_add[['고객번호','성별구분','연령대','나이','직업위험등급','직업대분류코드'
                 ,'배우자여부','자녀여부','설계사수' ,'경제활동참가율'#'지점수','설계사수'
                 ,'스타벅스매장수','2020_건강생활실천율','시도명','상품분류코드_1'#,'유지계약건수','성립계약건수'
                 #'상급종합병원','2020_건강생활실천율','2020_출생률' ,'평균연령'
                 # 'FP몰선호상품코드','모집대면비대면여부',
                 #,'2020_노인여가복지시설수' ,'학문/교육' 
                ]] 
                

df_cos_label = df_cos.copy()
# 직업대분류 코드 LABEL 화 
df_cos_label = pd.get_dummies(df_cos_label, columns = ['직업대분류코드'])
df_cos = df_cos_label.copy()
# temp DF생성 
df_add_temp = df_add[['고객번호','상품분류코드_1']]
df_add_cus_index = df_add_temp.set_index('고객번호')
df2 = df_add_temp.copy()

#최근 많이 판매된 상품 top3 구하기
df_add.head(2)
fil_sal = (df_add.계약월_1 >= 202106)   #data_final
df_add_top_sal = df_add.loc[fil_sal,:] 
df_add_top_sal.groupby('계약월_1').sum()
df_top_sal = df_add_top_sal['상품분류코드_1']
top_sal_list  = []
top_sal_list = df_top_sal.value_counts().index[:3]
top_sal_list
df_top_sal.value_counts()

#knn data read
# 중요 FEATURE 추출
df_knn = df_add[['고객번호','성별구분','나이','직업위험등급','직업대분류코드'
                 ,'배우자여부','자녀여부','설계사수','보건진료소' #'지점수','설계사수'
                 ,'스타벅스매장수','2020_건강생활실천율','시도명', '경제활동참가율'
                 #'상급종합병원','2020_건강생활실천율','2020_출생률' # 경제활동참가율
                 # 'FP몰선호상품코드','모집대면비대면여부',,'평균연령'
                 #,'2020_노인여가복지시설수' ,'학문/교육' 
                ]] 
# 직업대분류 코드 LABEL 화 
df_knn = pd.get_dummies(df_knn, columns = ['직업대분류코드'])
# 시도명 label화
df_knn = pd.get_dummies(df_knn, columns = ['시도명'])
df_knn_copy = df_knn[['고객번호']]
df_knn_copy.to_csv('df_knn_copy_random.csv',index = False)
df_knn = df_knn.drop(['고객번호'],axis = 1)

# 정규화 진행
scaler = MinMaxScaler()
data_scale = scaler.fit_transform(df_knn)

# 그룹수를 50개 정도로 조정 
k = 50

# 그룹 수, random_state 설정
model = KMeans(init = 'random' , n_clusters = k ) #init = 'k-means++'random random_state = 10

# 정규화된 데이터에 학습
model.fit(data_scale)

# 클러스터링 결과 각 데이터가 몇 번째 그룹에 속하는지 저장
df_knn['cluster'] = model.fit_predict(data_scale)


#data 추출
def make_data (df_cos, option ,sex,age,addr,k,size):
    if option == 'human':
        # 들어온 값을 시도 한글로 변경 
        if addr == 0 :
            addr = '서울특별시' 
        elif addr == 1 :
            addr = '부산광역시'         
        elif addr == 2 :
            addr = '대구광역시'
        elif addr == 3 :
            addr = '인천광역시'
        elif addr == 4 :
            addr = '광주광역시'
        elif addr == 5 :
            addr = '대전광역시'
        elif addr == 6 :
            addr = '울산광역시'
        elif addr == 7 :
            addr = '세종특별자치시'
        elif addr == 8 :
            addr = '경기도'
        elif addr == 9 :
            addr = '강원도'
        elif addr == 10 :
            addr = '충청북도'            
        elif addr == 11 :
            addr = '충청남도'  
        elif addr == 12 :
            addr = '전라북도'              
        elif addr == 13 :
            addr = '전라남도'             
        elif addr == 14 :
            addr = '경상북도'              
        elif addr == 15 :
            addr = '경상남도'             
        elif addr == 16 :
            addr = '제주특별자치도'
        else:
            print('시도 error : ', addr)
          
        print('성별: {0}, 연령대 : {1}, 시도명 : {2}'.format(sex,age,addr))
        #fil_yn = '##'
        #if age >= 60:
        #    fil_yn = (df_cos.성별구분 == sex) & (df_cos.연령대 >= age)  & (df_cos.시도명 == addr)  
        #else: 
        fil_yn = (df_cos.성별구분 == sex) & (df_cos.연령대 == age)  & (df_cos.시도명 == addr)  #data_final
        filename = 'df_cos_{0}_{1}_{2}'.format(sex,age,addr)
       # print(filename)
        data = df_cos.loc[fil_yn,:]
        data = data.drop(['성별구분','연령대','시도명'],axis = 1 )
    elif option == 'knn':
        fil_yn = (df_knn.cluster == k)   #data_final
        filename = df_knn.loc[fil_yn,:]   
        data = pd.merge(left = filename , right = df_knn_copy, left_index=True, right_index=True, how='left')
        data = data.drop(['cluster'],axis=1)
    else :
        data = pd.get_dummies(df_cos, columns = ['시도명'])
        data = data.sample(n=size,replace=False) # replace=True 비복원추출 
    #print("data.shape",data.shape)    
    return data    
 
 def cos_reco(data):
    #print("data_shape",data.shape)
    #----------cosine 유사도를 구하기 위해 StandardScaler start -----------------------
    data = data.set_index('고객번호',drop=True)
    print("StandardScaler start! ")
    # StandardScaler 사용 
    # 객체생성
    scaler = StandardScaler()

    # fit -> transform
    scaler.fit(data) # df는 2차원 이상의 값이어야 함
    df_scaled = scaler.transform(data)

    # 배열형태로 반환되기 때문에 데이터 프레임으로 변환해주는 작업
    df_scaled = pd.DataFrame(data = df_scaled, index=data.index, columns=data.columns)

    # *** 만약 특정 열의 스케일링을 하고 싶은 경우 ***
    #A_n = scaler.fit_transform(df['A'].values.reshape(-1,1))
    #df.insert(0, 'A_scaled', A_n)
    #df.drop(['A'], axis=1, inplace=True)
    
    # 파일 copy 
    df = df_scaled.copy()
    print("StandardScaler End! ")
    #----------cosine 유사도를 구하기 위해 StandardScaler end -----------------------
    
    
    
    #----------cosine 유사도 start --------------------------------------------------
    print("cosine_similarity start! ")
    item_sim = cosine_similarity(df, df)

    # cosine_similarity() 로 반환된 넘파이 행렬을 영화명을 매핑하여 DataFrame으로 변환
    item_sim_df = pd.DataFrame(data=item_sim, index=df.index,
                           columns=df.index)
    #print(item_sim_df.shape)
    item_sim_df.head(10)
    
    df = item_sim_df.copy()
    print("cosine_similarity End! ")
    #----------cosine 유사도 end----0-------------------------------------------------
    
    #----------유사한 고객 산출 start--------------------------------------------------
    print("cosine_customer start! ")
    # 30개 추출 
    dic = {}
    for 고객번호 in df.columns:
        유사집단 = df.loc[int(고객번호), df.columns != 고객번호].sort_values(
            ascending=False).head(30).index.tolist()
        dic[고객번호]=유사집단
        
    df_tec = pd.DataFrame(dic).transpose()    
    #print('df_tec.shape',df_tec.shape)
    df = df_tec.copy()
    print("cosine_customer end! ")
    #----------유사한 고객 산출 end---------------------------------------------------

    #----------유사한 고객의 상품코드 추출 start -------------------------------------
    print("cosine_prod start! ")
    df = df.iloc[:,:]
    dic = {}
    keys = df.columns.tolist()
    for key in keys:
        dic[key] = df[key].tolist()
    
    고객번호_dic = {}
    고객번호_list = []
    keys = df.columns.tolist()
    #for index, 고객번호 in enumerate(df['고객번호'].tolist()):
    for index, 고객번호 in enumerate(df.index.tolist()):
        고객번호_dic[고객번호] = df.iloc[index].tolist()
    
    recommendation_dic = {}
    for key in 고객번호_dic.keys():
        #print(f'고객번호 : {key}')
        recommendation_list = []
        for index in range(len(고객번호_dic[key])):
            recommendation = df2.loc[df2['고객번호']==고객번호_dic[key][index], '상품분류코드_1'].values[0]
            recommendation_list.append(recommendation)
        recommendation_dic[key] = recommendation_list
   # print('df_rec.shape',df_rec.shape)
    pd.DataFrame(recommendation_dic).transpose()
    df_rec = pd.DataFrame(recommendation_dic).transpose()
    print("cosine_prod end! ")
    #----------유사한 고객의 상품코드 추출 end ----------------------------------------
    
    
    #----------유사한 고객의 상품코드 최빈값 start -------------------------------------
    print("customer mode start! ")
    # mode 최빈값 여러건인지 확인하기 
    df_rec['추천코드_1'] = '0'
    df_rec['추천코드_2'] = '0'
    df_rec['추천코드_3'] = '0'
    df_rec['추천코드_top'] = '0'
    # top_sal_list
    k = 1 
    for i in range (0,len(df_rec)):
        list = []
        list = str(statistics.mode(df_rec.iloc[i]))
        #print(list)
        
        df_rec["추천코드_1"].iloc[i] = str(statistics.mode(df_rec.iloc[i]))
        #print(str(statistics.mode(df_rec.iloc[i])))
        # 두번째 추천코드 
        # 두번째 추천코드
        if str(statistics.mode(df_rec.iloc[i])) != '0' :
            fil = (statistics.mode(df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]) 
                   != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i])
            prd_2 = df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i][fil]
            if len(prd_2) != 0:
                df_rec["추천코드_2"].iloc[i]= str(statistics.mode(prd_2))
                # -----3번쨰 상품 추천을 위해 
                fil3 = ((statistics.mode(df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]) 
                        != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i]))
                fil4 = statistics.mode(prd_2) != df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i] 
            
                fil_3 = ( fil3 == fil4)  
                prd_3 =  df_rec.drop(['추천코드_1','추천코드_2','추천코드_3','추천코드_top'],axis=1).iloc[i][fil_3]
                
                if len(prd_3)!= 0:
                    df_rec["추천코드_3"].iloc[i]= str(statistics.mode(prd_3))
                    #print(str(statistics.mode(prd_3)))
                else:
                    df_rec["추천코드_3"].iloc[i] = 0
                
            else :
                df_rec["추천코드_2"].iloc[i] = 0
        else :
            df_rec["추천코드_2"].iloc[i] = 0
        

        for u in range (0,3):
                     
            if (df_rec["추천코드_1"].iloc[i] != str(top_sal_list[u])) and (df_rec["추천코드_2"].iloc[i] != str(top_sal_list[u])):
                df_rec['추천코드_top'].iloc[i] =  str(top_sal_list[u]) 
                break
                
        
        #if str(statistics.mode(df_rec.iloc[i])) != '0' :
        #    #print("aaa")
        #    fil = (statistics.mode(df_rec.iloc[i]) != df_rec.iloc[i])
        #    prd_2 = df_rec.iloc[i][fil]
        #    df_rec["추천코드_2"].iloc[i]= str(statistics.mode(prd_2))
        #else :
        #    df_rec["추천코드_2"].iloc[i] = 0
        #    
        if len(list) > 3 :
            print("2개이상" , i)
            k = k + 1
        i = i+1
    #print(k)
    print("customer mode end! ")
    #----------유사한 고객의 상품코드 최빈값 end -------------------------------------
    
    #-----------추천 상품에 대한 정확도 측정 start ------------------------------------
   # print("correct start! ")
    
    df_t = pd.merge(left = df_rec , right = df_add_cus_index, left_index=True, right_index=True, how='left')
    
    df_t['일치여부_1'] = 0
    df_t['일치여부_2'] = 0
    df_t['일치여부_3'] = 0   
    df_t['일치여부_top'] = 0
    match1_y = 0
    match2_y = 0
    match3_y = 0
    match_top_y = 0 
    for i in range (0,len(df_t)): #len(df_t)

        if str(df_t['추천코드_1'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
            df_t['일치여부_1'].iloc[i] = 1
            match1_y = match1_y+1
        else:
            df_t['일치여부_1'].iloc[i] = 0        
        
        if str(df_t['추천코드_2'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
            df_t['일치여부_2'].iloc[i] = 1
            match2_y = match2_y + 1
        else:
            df_t['일치여부_2'].iloc[i] = 0      
        
        if str(df_t['추천코드_3'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
            df_t['일치여부_3'].iloc[i] = 1
            match3_y = match3_y + 1
        else:
            df_t['일치여부_3'].iloc[i] = 0   
            
        if str(df_t['추천코드_top'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
            df_t['일치여부_top'].iloc[i] = 1
            match_top_y = match_top_y + 1
        else:
            df_t['일치여부_top'].iloc[i] = 0              
    #print(match1_y/len(df_t), '/',match1_y, len(df_t))
    #print(match2_y/len(df_t), '/',match2_y, len(df_t))
    #print("correct end! ")
    return match1_y, match2_y,match3_y,match_top_y, len(df_t)
    #return df_t 
    
    #-----------추천 상품에 대한 정확도 측정 end ------------------------------------
    
#모델별 정확도 측정    
df_knn_result = pd.DataFrame(columns=['idx', 'knn_cluster','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate'])

#k-nn 클러스터별 정확도 측정
for i in range (0,50):
    corr_rate = 0 
    data = make_data(df_knn,'knn',0 ,0 ,0,i,0) ## 앞에 어느 값을 줄지 선택 
    match1_y, match2_y, match3_y,match_top_y, df_len = cos_reco(data) 
    knn_cluster = i 
    idx = i 
    corr_rate  = round((int(match1_y)+int(match2_y)+int(match3_y)) / int(df_len), 6) 
    df_knn_result = df_knn_result.append(pd.DataFrame([[idx, knn_cluster,match1_y,match2_y,match3_y,match_top_y,df_len,corr_rate]],
                                   columns=['idx', 'knn_cluster','match1_y','match2_y','match3_y','match_top_y','df_len','corr_rate']), 
                                   ignore_index=True)
    #print('i',i)
    
#Model기반 추천 시스템   
# 중요 FEATURE 추출
df_DT  = df_add[['고객번호','성별구분','나이','직업위험등급','직업대분류코드'
                 ,'배우자여부','자녀여부','설계사수','보건진료소' #'지점수','설계사수'
                 ,'스타벅스매장수','2020_건강생활실천율','시도명', '경제활동참가율'
                 ,'상품분류코드_1'
                 #'상급종합병원','2020_건강생활실천율','2020_출생률' ,,'평균연령'
                 # 'FP몰선호상품코드','모집대면비대면여부',
                 #,'2020_노인여가복지시설수' ,'학문/교육' 
                ]] 
                
# 직업대분류 코드 LABEL 화 
df_DT = pd.get_dummies(df_DT, columns = ['직업대분류코드'])
# 시도명 label화
df_DT = pd.get_dummies(df_DT, columns = ['시도명'])               

df_y = df['상품분류코드_1']

# 인덱스를 고객번호로
df = df.set_index('고객번호')
from sklearn.model_selection import train_test_split
x_train, x_valid, y_train, y_valid = train_test_split(df_x, df_y, test_size=0.3)

#hyper parameter
import optuna
import sklearn
def objective(trial):
    # Define the search space
    criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])
    max_depths = trial.suggest_int('max_depth', 1, 9, 1)
   # n_estimators = trial.suggest_int('n_estimators', 1, 20, 1)
    max_leaf_nodes = int(trial.suggest_int("max_leaf_nodes", 2, 50))

    clf = DecisionTreeClassifier(#n_estimators=n_estimators,
                                 max_leaf_nodes = max_leaf_nodes,                 
                                 criterion=criterions,
                                 max_depth=max_depths)
                               #  n_jobs=-1)
                                 
    score = cross_val_score(clf, x_train, y_train, scoring="accuracy").mean()

    return score

# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계
# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정
study = optuna.create_study()
study.optimize(objective, n_trials=50)

trial = study.best_trial

print('Accuracy: {}'.format(trial.value))
print("Best hyperparameters: {}".format(trial.params))
# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드
print(study.best_trial.params)

# 시행된 trial 중 가장 높은 값 반환하는 메소드
optuna_acc = study.best_trial.value
print(optuna_acc)

#DecisionTreeClassifier
# DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(max_depth=6, max_leaf_nodes = 36, splitter='best', criterion='entropy')
dtc.fit(x_train, y_train)
dtc_pred = dtc.predict(x_valid)
(dtc_pred == y_valid).mean()

# df for valid(test)
df_valid = x_valid 

# 상품분류코드 idx
dtc.classes_

item_code = {0: 111,
             1: 120,
             2: 130,
             3: 141,
             4: 142,
             5: 150,
             6: 162,
             7: 163,
             8: 164,
             9: 181,
             10: 182,
             11: 183,
             12: 192}
             
# predict_proba 분류기 확률값 치환
dtc_pred_proba = dtc.predict_proba(x_valid)
dtc_pred_proba

# 검증, 첫번째 분류기 output값이 150이 맞는지 확인해보자
a = dtc_pred_proba[0]

# np.argpartition(sth, 2) -> 순서 상관 없이 2번째까지 작은 숫자 idx 뽑아서 왼쪽으로 놓는다
# np.argpartition(sth, -2) -> 순서 상관 없이 2번째까지 큰 숫자 idx 뽑아서 오른쪽으로 놓는다

# 2번째까지 큰 숫자 idx 뽑아서 오른쪽으로 놓고
# 오른쪽 2개를 idx로 가져온다
idx = np.argpartition(a, -3)[-3:] # idx = [rank2, rank1]

# idx는 상품분류코드 번호를 뜻하게 된다
# reverse -> [rank1, rank2]
idx = np.sort(idx)[::-1]
idx

# df_valid(test) 생성
df_valid = x_valid.copy()

# rank 컬럼 생성
df_valid['rank1']=''
df_valid['rank2']=''
df_valid['rank3']=''

for index, 고객번호 in enumerate(df_valid.index):
    추천상품 = np.argpartition(dtc_pred_proba[index], -3)[-3:]
    df_valid.loc[고객번호, 'rank1'] = 추천상품[0]
    df_valid.loc[고객번호, 'rank2'] = 추천상품[1]
    df_valid.loc[고객번호, 'rank3'] = 추천상품[2]
   
# 상품분류코드로 변환하기
df_valid['rank1'] = df_valid['rank1'].replace(item_code)
df_valid['rank2'] = df_valid['rank2'].replace(item_code)
df_valid['rank3'] = df_valid['rank3'].replace(item_code)

df_DT_cus = df_add[['고객번호','상품분류코드_1']]

df_DT_cus = df_DT_cus.set_index('고객번호')
df_valid_DT = df_valid.set_index('고객번호')

df_t = pd.merge(left = df_valid_DT , right = df_DT_cus, left_index=True, right_index=True, how='left')

df_t['일치여부_1'] = 0
df_t['일치여부_2'] = 0
df_t['일치여부_3'] = 0
match1_y = 0
match2_y = 0
match3_y = 0
for i in range (0,len(df_t)): #len(df_t)

    if str(df_t['rank1'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
        df_t['일치여부_1'].iloc[i] = 1
        match1_y = match1_y+1
    else:
        df_t['일치여부_1'].iloc[i] = 0        
    
    if str(df_t['rank2'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
        df_t['일치여부_2'].iloc[i] = 1
        match2_y = match2_y + 1
    else:
        df_t['일치여부_2'].iloc[i] = 0 

    if str(df_t['rank3'].values[i]) == str(df_t['상품분류코드_1'].values[i]):
        df_t['일치여부_3'].iloc[i] = 1
        match3_y = match3_y + 1
    else:
        df_t['일치여부_3'].iloc[i] = 0 
        
#Random Forest
import optuna
import sklearn
def objective(trial):
    # Define the search space
    criterions = trial.suggest_categorical('criterion', ['gini', 'entropy'])
    max_depth = trial.suggest_int('max_depth', 1, 10)
    max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 2, 1000)
    n_estimators =  trial.suggest_int('n_estimators', 100, 500)

    clf = sklearn.ensemble.RandomForestClassifier( criterion=criterions,
                                 n_estimators=n_estimators,
                                 max_leaf_nodes = max_leaf_nodes,                 
                                 max_depth=max_depth,
                                 n_jobs=-1,
                                 random_state=0                 
                                                 )
                                 
    score = cross_val_score(clf, x_train, y_train, scoring="accuracy").mean()

    return score

# 3. study 오브젝트 생성하고 목적함수 최적화하는 단계
# 여기서는 목적함수를 정확도로 설정했기 때문에 최대화를 목표로 하고 있지만, 손실함수의 경우 direction='minimize'로 설정
study = optuna.create_study()
study.optimize(objective, n_trials=50)

trial = study.best_trial

print('Accuracy: {}'.format(trial.value))
print("Best hyperparameters: {}".format(trial.params))

# 시행된 trial 중 최적의 하이퍼파라미터 반환하는 메소드
print(study.best_trial.params)

# 시행된 trial 중 가장 높은 값 반환하는 메소드
optuna_acc = study.best_trial.value
print(optuna_acc)

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(max_depth=8,max_leaf_nodes = 733, n_estimators= 500, criterion='entropy', random_state=0) 
# (n_estimators=474, max_depth=1, max_leaf_nodes=552, random_state=25)
rfc.fit(x_train, y_train)
rfc_pred = rfc.predict(x_valid)
(rfc_pred == y_valid).mean()

rfc.classes_
# predict_proba 분류기 확률값 치환
rfc_pred_proba = rfc.predict_proba(x_valid)
rfc_pred_proba

for index, 고객번호 in enumerate(df_valid.index):
    추천상품 = np.argpartition(rfc_pred_proba[index], -3)[-3:]
    df_valid.loc[고객번호, 'rank1'] = 추천상품[0]
    df_valid.loc[고객번호, 'rank2'] = 추천상품[1]
    df_valid.loc[고객번호, 'rank3'] = 추천상품[2]
    
# 상품분류코드로 변환하기
df_valid['rank1'] = df_valid['rank1'].replace(item_code)
df_valid['rank2'] = df_valid['rank2'].replace(item_code)
df_valid['rank3'] = df_valid['rank3'].replace(item_code)

df_t1 = pd.merge(left = df_valid_RF , right = df_DT_cus, left_index=True, right_index=True, how='left')

df_t1['일치여부_1'] = 0
df_t1['일치여부_2'] = 0
df_t1['일치여부_3'] = 0

match1_y = 0
match2_y = 0
match3_y = 0

for i in range (0,len(df_t1)): #len(df_t)

    if str(df_t1['rank1'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):
        df_t1['일치여부_1'].iloc[i] = 1
        match1_y = match1_y+1
    else:
        df_t1['일치여부_1'].iloc[i] = 0        
    
    if str(df_t1['rank2'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):
        df_t1['일치여부_2'].iloc[i] = 1
        match2_y = match2_y + 1
    else:
        df_t1['일치여부_2'].iloc[i] = 0 
        
    if str(df_t1['rank3'].values[i]) == str(df_t1['상품분류코드_1'].values[i]):
        df_t1['일치여부_3'].iloc[i] = 1
        match3_y = match3_y + 1
    else:
        df_t1['일치여부_3'].iloc[i] = 0     
